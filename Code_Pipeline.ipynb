{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_BA_Code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIOibXnEVm5h",
        "colab_type": "text"
      },
      "source": [
        "# **Bachelor Thesis: Twitter Text-Mining Tool**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2BDFyAIX8yi",
        "colab_type": "text"
      },
      "source": [
        "## **Code Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEen7z7DWoNn",
        "colab_type": "text"
      },
      "source": [
        "**Installing Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TG3uz45dbY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pathlib\n",
        "!pip install patool\n",
        "!pip install internetarchive\n",
        "!pip install elasticsearch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2WZg2sdWuaD",
        "colab_type": "text"
      },
      "source": [
        "**Setup Variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpfZMtIxddWU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define path names\n",
        "path_to_files = '/content'\n",
        "\n",
        "# Name result files \n",
        "rf_key_tweets = 'key_tweets.json'\n",
        "rf_extracted_tweets = 'extracted_tweets.json'\n",
        "rf_all_extracted_tweets = 'all_extracted_tweets.json'\n",
        "\n",
        "# Define identifiers to download datasets\n",
        "identifier_list = ['archiveteam-twitter-stream-2017-11', 'archiveteam-twitter-stream-2018-01', 'archiveteam-twitter-stream-2018-02']\n",
        "\n",
        "# Define index name for Elasticsearch\n",
        "es_index_name = 'tweet_results_2017-11'\n",
        "\n",
        "# Define searchterms\n",
        "st_bitcoin = (\"Bitcoin\", \"bitcoin\", \"BTC\", \"btc\")\n",
        "st_litecoin = (\"Litecoin\", \"Litecoin\", \"LTC\", \"ltc\")\n",
        "st_ripple = (\"Ripple\", \"ripple\", \"XRP\", \"xrp\")\n",
        "searchterm = st_bitcoin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUC3AxfpYB77",
        "colab_type": "text"
      },
      "source": [
        "## **Code Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHxWdviEYaqr",
        "colab_type": "text"
      },
      "source": [
        "**Download Data With Identifiers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcS2uSnfdqml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code cell downloads all dataset within the predefined identifier list to a predefined path\n",
        "\n",
        "import internetarchive\n",
        "from internetarchive import configure\n",
        "from internetarchive import get_session\n",
        "from internetarchive import get_item\n",
        "from internetarchive import get_files\n",
        "\n",
        "def downloader(identifier_list, path):\n",
        "    # Grab all files in the identifier\n",
        "    ident_list = identifier_list\n",
        "    f_path = path\n",
        "\n",
        "    print(\"1. Download files ...\")\n",
        "    print(\"     Number of Identifiers: \", len(ident_list))\n",
        "\n",
        "    # Config File & Configuration\n",
        "    user = 'lsonthi@web.de'\n",
        "    pw = 'bachelor'\n",
        "    configure(user, pw)\n",
        "    config = dict(s3=dict(access='data_download', secret='bar'))\n",
        "    s = get_session(config)\n",
        "    s.access_key\n",
        "\n",
        "    # Download all tar files from IA objects\n",
        "    for ident in ident_list:\n",
        "        item = s.get_item(ident)\n",
        "        file_names = [f.name for f in get_files(ident, glob_pattern='*tar')]\n",
        "        for i in file_names:\n",
        "            item.download(files=i)\n",
        "            \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c56Fe8pKYT_A",
        "colab_type": "text"
      },
      "source": [
        "**Extract Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKnYK4J1dkkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code cell extracts in 2 steps all datasets to a predefined path: \n",
        "# 1) Extract all TAR files to get BZ2 files\n",
        "# 2) Extract all BZ2 files to get JSON files + analyze tweet text from datasets\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import patoolib\n",
        "from pathlib import Path\n",
        "\n",
        "# Unzips all tar files\n",
        "def extract_tar(path):\n",
        "    path_to_files = path\n",
        "    \n",
        "    print(\"2. Unzip TAR files ...\")\n",
        "    \n",
        "    for item in glob.glob(path_to_files + '/*.tar'): \n",
        "        dirpath = os.path.dirname(item)\n",
        "        patoolib.extract_archive(item, outdir=dirpath)\n",
        "        os.remove(os.fspath(item))\n",
        "    return\n",
        "\n",
        "# Unzips all bz2 files from the folders\n",
        "def extract_bz2(path, searchterm, rf_name):\n",
        "    path_to_files = path\n",
        "    st = searchterm\n",
        "    result_file = rf_name\n",
        "    tweet_array = []\n",
        "    \n",
        "    print(\"3. Extracting BZ2 files ...\")\n",
        "    \n",
        "    for item in glob.glob(path_to_files + '/**/*.bz2', recursive=True):\n",
        "        dirpath = os.path.dirname(item)\n",
        "        patoolib.extract_archive(item, outdir=dirpath)\n",
        "        \n",
        "        # Analyze tweets with function analyze_tweet_text()\n",
        "        tweet_array = analyze_tweet_text(path_to_files, st)\n",
        "        \n",
        "        # Write key tweets to result file\n",
        "        with open('key_tweets.json', 'w', encoding=\"utf-8\") as file:\n",
        "          for it in tweet_array:\n",
        "            file.write(\"%s\\n\" % json.dumps(it))\n",
        "        \n",
        "        os.remove(os.fspath(item))\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fML-zHD4Yz2q",
        "colab_type": "text"
      },
      "source": [
        "**Analyze Tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iHZcZ7qeqz8c",
        "colab": {}
      },
      "source": [
        "# This code cell filters JSON files for tweets which contain the one of the terms in the predefined searchterm list and extraxts only the key attributes from relevant tweets\n",
        "# Fuction analyze_tweet_text() gets called in the function extract_bz2()\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from glob import iglob\n",
        "\n",
        "def analyze_tweet_text(path, searchterms):\n",
        "    path_to_json = path\n",
        "    st = searchterms\n",
        "    tweets_final = [] # Array for final Tweets\n",
        "    \n",
        "    print(\"4. Analyze Tweets ...\")\n",
        "\n",
        "    # Search for JSON files\n",
        "    print(\"     Importing data ...\")\n",
        "    rootdir = Path(path_to_json)\n",
        "    json_files = list(rootdir.glob('**/*.json')) # List with all JSON Files\n",
        "\n",
        "    # Start filter process\n",
        "    print(\"     Filter process started ...\")\n",
        "    for index, js in enumerate(json_files):\n",
        "        with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
        "            for line in json_file:\n",
        "                if line.strip():\n",
        "                    tweet_line = json.loads(line)\n",
        "\n",
        "                # 1. Filter: Check for deleted tweets\n",
        "                    if 'source' in tweet_line:\n",
        "                    #  print(\"Tweet exists\")\n",
        "\n",
        "                # 2. Filter: Check if tweet has more than 140 characters (truncated = true)\n",
        "                            if tweet_line['truncated'] == True:\n",
        "                                tweet_text = tweet_line['extended_tweet']['full_text']\n",
        "                            else:\n",
        "                                tweet_text = tweet_line['text']\n",
        "\n",
        "                # 3. Filter: Check if text contains any of the searchterms\n",
        "                            if any(s in tweet_text for s in st):\n",
        "                                key_tweet = []\n",
        "                                \n",
        "                                try:\n",
        "                                  key_tweet.append(tweet_line)\n",
        "                                  tweets_final.append(extract_key_info(key_tweet))\n",
        "                                  print(len(tweets_final))\n",
        "                                except ValueError:\n",
        "                                  print(\"Decoding JSON has failed\")\n",
        "                            else:\n",
        "                                continue\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "    return tweets_final"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvHe8MJoYH8q",
        "colab_type": "text"
      },
      "source": [
        "**Extract Key Information**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cFVE6BVdfvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code cell extracts only the key attributes from the key files \n",
        "# Fuction extract_key_info() gets called in the function analyze_tweet_text()\n",
        "\n",
        "import json\n",
        "\n",
        "def extract_key_info(tweets):\n",
        "    raw_tweets = tweets # Array for key tweets\n",
        "    extracted_tweets = [] # Array for extracted tweets\n",
        "    \n",
        "    # Extract Date, ID, Text, User-ID, User-Name and User-Timezone\n",
        "    for i in range(len(raw_tweets)):\n",
        "        # Text Attributes - Check if text contains more than 140 characters\n",
        "        if raw_tweets[i]['truncated'] == True:\n",
        "            tweet_text = raw_tweets[i]['extended_tweet']['full_text']\n",
        "        else:\n",
        "            tweet_text = raw_tweets[i]['text']\n",
        "\n",
        "        # Tweet Key Values\n",
        "        tweet_date = raw_tweets[i]['created_at']\n",
        "        tweet_id = raw_tweets[i]['id']\n",
        "        tweet_source = raw_tweets[i]['source']\n",
        "\n",
        "        # Tweet User Attributes\n",
        "        tweet_user_id = raw_tweets[i]['user']['id']\n",
        "        tweet_user_name = raw_tweets[i]['user']['name']\n",
        "        tweet_user_location = raw_tweets[i]['user']['location']\n",
        "        tweet_user_url =  raw_tweets[i]['user']['url']\n",
        "        tweet_user_description = raw_tweets[i]['user']['description']\n",
        "        tweet_user_verified = raw_tweets[i]['user']['verified']\n",
        "        tweet_user_follower_count = raw_tweets[i]['user']['followers_count']\n",
        "        tweet_user_friends_count = raw_tweets[i]['user']['friends_count']\n",
        "        tweet_user_favourites_count = raw_tweets[i]['user']['favourites_count']\n",
        "        tweet_user_statuses_count = raw_tweets[i]['user']['statuses_count']\n",
        "        tweet_user_created_at = raw_tweets[i]['user']['created_at']\n",
        "        tweet_user_utc_offset = raw_tweets[i]['user']['utc_offset']\n",
        "        tweet_user_timezone = raw_tweets[i]['user']['time_zone']\n",
        "        tweet_user_geo_enabled = raw_tweets[i]['user']['geo_enabled']\n",
        "        tweet_user_language = raw_tweets[i]['user']['lang']\n",
        "\n",
        "        # Tweet Attributes\n",
        "        tweet_geo = raw_tweets[i]['geo']\n",
        "        tweet_coordinates = raw_tweets[i]['coordinates']\n",
        "        tweet_place = raw_tweets[i]['place']\n",
        "        tweet_quote_count = raw_tweets[i]['quote_count']\n",
        "        tweet_reply_count = raw_tweets[i]['reply_count']\n",
        "        tweet_retweet_count = raw_tweets[i]['retweet_count']\n",
        "        tweet_favorite_count = raw_tweets[i]['favorite_count']\n",
        "        #tweet_hastags = raw_tweets[i]['entities']['hastags']\n",
        "        #tweet_urls = raw_tweets[i]['entities']['urls']\n",
        "        tweet_favorited = raw_tweets[i]['favorited']\n",
        "        tweet_retweeted = raw_tweets[i]['retweeted']\n",
        "        tweet_language = raw_tweets[i]['lang']\n",
        "        tweet_timestamp = raw_tweets[i]['timestamp_ms']\n",
        "\n",
        "        # Create a new JSON-Object structure\n",
        "        jsonobj = {\n",
        "            \"created_at\": tweet_date,\n",
        "            \"id\": tweet_id,\n",
        "            \"text\": tweet_text,\n",
        "            \"source\": tweet_source,\n",
        "            \"user\": {\n",
        "                \"id\": tweet_user_id,\n",
        "                \"name\": tweet_user_name,\n",
        "                \"location\": tweet_user_location,\n",
        "                \"url\": tweet_user_url,\n",
        "                \"description\": tweet_user_description,\n",
        "                \"verified\": tweet_user_verified,\n",
        "                \"followers_count\": tweet_user_follower_count,\n",
        "                \"friends_count\": tweet_user_friends_count,\n",
        "                \"favourites_count\": tweet_user_favourites_count,\n",
        "                \"statuses_count\": tweet_user_statuses_count,\n",
        "                \"created_at\": tweet_user_created_at,\n",
        "                \"utc_offset\": tweet_user_utc_offset,\n",
        "                \"time_zone\": tweet_user_timezone,\n",
        "                \"geo_enabled\": tweet_user_geo_enabled,\n",
        "                \"lang\": tweet_user_language,\n",
        "                },\n",
        "            \"geo\": tweet_geo,\n",
        "            \"coordinates\": tweet_coordinates,\n",
        "            \"place\": tweet_place,\n",
        "            \"quote_count\": tweet_quote_count,\n",
        "            \"reply_count\": tweet_reply_count,\n",
        "            \"retweet_count\": tweet_retweet_count,\n",
        "            \"favorite_count\": tweet_favorite_count,\n",
        "            #\"hastags\": tweet_hastags,\n",
        "            #\"urls\": tweet_urls,\n",
        "            \"favorited\": tweet_favorited,\n",
        "            \"retweeted\": tweet_retweeted,\n",
        "            \"lang\": tweet_language,\n",
        "            \"timestamp_ms\": tweet_timestamp,\n",
        "        }\n",
        "\n",
        "        extracted_tweets.append(jsonobj)\n",
        "\n",
        "    return extracted_tweets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2L2klCiYmoB",
        "colab_type": "text"
      },
      "source": [
        "**Index Result Files To Elasticsearch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erNXsWRZdmZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code cell connects to Elasticsearch, creates an index with a predefined index name and fills it with the extracted tweets\n",
        "\n",
        "from elasticsearch import Elasticsearch\n",
        "import json\n",
        "\n",
        "def index_to_es(index_name, json_file):\n",
        "  index_n = index_name\n",
        "  json_f = json_file\n",
        "  print(\"7. Index Tweets to Elasticsearch ...\")\n",
        "\n",
        "  # Connect to the elastic cluster\n",
        "  es = Elasticsearch(['https://search-my-ba-cluster-ckk6c27ovy52lt7e4pjk76yfuu.eu-central-1.es.amazonaws.com'])\n",
        "  print(es)\n",
        "\n",
        "  # Setup the settings and mapping\n",
        "  settings = {\n",
        "      \"settings\": {\n",
        "          \"index.mapping.total_fields.limit\": 80000,\n",
        "          \"number_of_shards\": 5,\n",
        "          \"number_of_replicas\": 1\n",
        "      },\n",
        "    \"mappings\": {\n",
        "      \"tweet\": {\n",
        "        \"properties\": {\n",
        "          \"created_at\": {\n",
        "            \"type\": \"date\",\n",
        "            \"format\": \"EEE MMM dd HH:mm:ss Z yyyy\",\n",
        "            \"fields\": {\n",
        "              \"keyword\": {\n",
        "                \"type\": \"keyword\",\n",
        "                \"ignore_above\": 256\n",
        "              }\n",
        "            }\n",
        "          },\n",
        "          \"coordinates.coordinates\": {\n",
        "            \"type\": \"geo_point\"\n",
        "          },\n",
        "          \"place.bounding_box\": {\n",
        "            \"type\": \"geo_shape\",\n",
        "            \"coerce\": \"true\",\n",
        "            \"ignore_malformed\": \"true\"\n",
        "          },\n",
        "          \"user\": {\n",
        "            \"properties\": {\n",
        "              \"created_at\": {\n",
        "                  \"type\": \"date\",\n",
        "                  \"format\": \"EEE MMM dd HH:mm:ss Z yyyy\",\n",
        "                  \"fields\": {\n",
        "                      \"keyword\": {\n",
        "                          \"type\": \"keyword\",\n",
        "                          \"ignore_above\": 256\n",
        "                      }\n",
        "                  }\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  # Create a new index \n",
        "  es.indices.create(index=index_n, ignore=400, body=settings)\n",
        "  print(\"   Index created\")\n",
        "\n",
        "  # Fill created index with all extracted tweets\n",
        "  tweets = []\n",
        "\n",
        "  with open(json_f) as source:\n",
        "      for line in source:\n",
        "          if line.strip():\n",
        "              tweets.append(json.loads(line))\n",
        "\n",
        "  for i in range(len(tweets)):\n",
        "      res = es.index(index=index_n, ignore=400, doc_type='tweet',id=i,body=tweets[i])\n",
        "  print(\"   Index filled\")\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb6nUz7zXpmd",
        "colab_type": "text"
      },
      "source": [
        "## **Execute Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-IHn-LldrnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1) Download datasets\n",
        "downloader(identifier_list, path_to_files)\n",
        "\n",
        "# 2) Extract datasets to BZ2 files\n",
        "extract_tar(path_to_files)\n",
        "\n",
        "# 3) Extract BZ2 files to JSON files and analyze files for relevant tweets\n",
        "extract_bz2(path_to_files, searchterm, rf_key_tweets) #calls function analyze_tweet_text()\n",
        "\n",
        "# 4) Index relevant tweets to Elasticsearch\n",
        "index_to_es(es_index_name, rf_key_tweets)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
